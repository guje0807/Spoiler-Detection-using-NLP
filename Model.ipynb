{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7647c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re,html,json\n",
    "import seaborn as sns\n",
    "import nltk as nt\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3963182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"final_df.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa9910",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed809c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text.\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text)\n",
    "    #remove numbers\n",
    "    text = re.sub(r'\\d+',\"\",text)\n",
    "    #lower\n",
    "    text = text.lower()\n",
    "    #tags like <tag>\n",
    "    text = re.sub(r'<[^<>]*>', ' ',text)\n",
    "    #Markdown Urls\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)',r'\\1',text)\n",
    "    #Remove Punctuation\n",
    "    text = re.sub(r'([!?,])\\1+', r'\\1', text)\n",
    "    #Remove all URL's\n",
    "    text = re.sub(r'http.*', ' ', text)\n",
    "    #Remove @\n",
    "    text = re.sub(r'@\\w*', ' ', text)\n",
    "    #text or code in brackets\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]',' ',text)\n",
    "    # remove b\"\n",
    "    text = text.replace('b\\\"',' ') \n",
    "    # remove b'\n",
    "    text = text.replace(\"b\\'\",' ') \n",
    "    # remove \\\\n\n",
    "    text = text.replace('\\\\n',' ')\n",
    "    #Remove &amp\n",
    "    text = text.replace('&amp',' ') \n",
    "    # remove UTF-8 code like \\\\xe2\n",
    "    text = re.sub(r'(\\\\x(.){2})', ' ',text) \n",
    "    #Standalone sequences for specials\n",
    "    text = re.sub(r'(?:^|\\s)[;.\\'\\\"&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ',text)\n",
    "    #stand alone sequence of hyphens \n",
    "    text= re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ',text)\n",
    "    # Sequence of white spaces\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375d743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_contractions_text(text, contractions):\n",
    "    \"\"\"\n",
    "    This function normalizes english contractions.\n",
    "    \"\"\"\n",
    "    new_token_list = []\n",
    "    token_list = text.split()\n",
    "    for word_pos in range(len(token_list)):\n",
    "        word = token_list[word_pos]\n",
    "        first_upper = False\n",
    "        if word[0].isupper():\n",
    "            first_upper = True\n",
    "        if word.lower() in contractions:\n",
    "            replacement = contractions[word.lower()]\n",
    "            if first_upper:\n",
    "                replacement = replacement[0].upper()+replacement[1:]\n",
    "            replacement_tokens = replacement.split()\n",
    "            if len(replacement_tokens)>1:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "                new_token_list.append(replacement_tokens[1])\n",
    "            else:\n",
    "                new_token_list.append(replacement_tokens[0])\n",
    "        else:\n",
    "            new_token_list.append(word)\n",
    "    sentence = \" \".join(new_token_list).strip(\" \")\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02dd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_contractions(sentence_list):\n",
    "    contraction_list = json.loads(open('english_contractions.json', 'r').read())\n",
    "    norm_sents = []\n",
    "    print(\"Normalizing contractions\")\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        norm_sents.append(_normalize_contractions_text(sentence, contraction_list))\n",
    "    return norm_sents\n",
    "\n",
    "contraction_list = json.loads(open('english_contractions.json', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f82dfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "439f1263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Title</th>\n",
       "      <th>Spoiler_flag</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>doc_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The conclusion to the series hits some of the ...</td>\n",
       "      <td>/title/tt1201607/</td>\n",
       "      <td>0</td>\n",
       "      <td>After burying Dobby at the garden of the Shell...</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.106248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Lion King was pretty much my favourite mov...</td>\n",
       "      <td>/title/tt0110357/</td>\n",
       "      <td>1</td>\n",
       "      <td>The Lion King takes place in the Pride Lands o...</td>\n",
       "      <td>0.197134</td>\n",
       "      <td>0.404854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK, perhaps there is truth in the saying \"lost...</td>\n",
       "      <td>/title/tt0364569/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film begins in medias res, with the silhou...</td>\n",
       "      <td>0.115011</td>\n",
       "      <td>0.219557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top Gun has been an 80's staple since it first...</td>\n",
       "      <td>/title/tt1745960/</td>\n",
       "      <td>1</td>\n",
       "      <td>Over three decades after his time at TOPGUN, C...</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>0.262304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'll keep this brief: This is simply one of th...</td>\n",
       "      <td>/title/tt0060196/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film tells the story of three men who purs...</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>0.156379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review              Title  \\\n",
       "0  The conclusion to the series hits some of the ...  /title/tt1201607/   \n",
       "1  The Lion King was pretty much my favourite mov...  /title/tt0110357/   \n",
       "2  OK, perhaps there is truth in the saying \"lost...  /title/tt0364569/   \n",
       "3  Top Gun has been an 80's staple since it first...  /title/tt1745960/   \n",
       "4  I'll keep this brief: This is simply one of th...  /title/tt0060196/   \n",
       "\n",
       "   Spoiler_flag                                           Synopsis  \\\n",
       "0             0  After burying Dobby at the garden of the Shell...   \n",
       "1             1  The Lion King takes place in the Pride Lands o...   \n",
       "2             0  The film begins in medias res, with the silhou...   \n",
       "3             1  Over three decades after his time at TOPGUN, C...   \n",
       "4             0  The film tells the story of three men who purs...   \n",
       "\n",
       "   Cosine_Similarity  doc_similarity  \n",
       "0           0.158318        0.106248  \n",
       "1           0.197134        0.404854  \n",
       "2           0.115011        0.219557  \n",
       "3           0.423765        0.262304  \n",
       "4           0.084172        0.156379  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3f0b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Review'] = df['Review'].map(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42f583a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing contractions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 22879/22879 [00:01<00:00, 12999.70it/s]\n"
     ]
    }
   ],
   "source": [
    "df['Review'] = normalize_contractions(df['Review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "sw = stopwords.words( 'english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8778bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    l = []\n",
    "    for i in text.split():\n",
    "        if i not in sw:\n",
    "            l.append(i.strip('\\'\\\"'))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec7b7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['Review'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "506974c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Title</th>\n",
       "      <th>Spoiler_flag</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>doc_similarity</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the conclusion to the series hits some of the ...</td>\n",
       "      <td>/title/tt1201607/</td>\n",
       "      <td>0</td>\n",
       "      <td>After burying Dobby at the garden of the Shell...</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.106248</td>\n",
       "      <td>[conclusion, series, hits, strongest, emotiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the lion king was pretty much my favourite mov...</td>\n",
       "      <td>/title/tt0110357/</td>\n",
       "      <td>1</td>\n",
       "      <td>The Lion King takes place in the Pride Lands o...</td>\n",
       "      <td>0.197134</td>\n",
       "      <td>0.404854</td>\n",
       "      <td>[lion, king, pretty, much, favourite, movie, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok, perhaps there is truth in the saying \"lost...</td>\n",
       "      <td>/title/tt0364569/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film begins in medias res, with the silhou...</td>\n",
       "      <td>0.115011</td>\n",
       "      <td>0.219557</td>\n",
       "      <td>[ok,, perhaps, truth, saying, lost, translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>top gun has been an 's staple since it first r...</td>\n",
       "      <td>/title/tt1745960/</td>\n",
       "      <td>1</td>\n",
       "      <td>Over three decades after his time at TOPGUN, C...</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>0.262304</td>\n",
       "      <td>[top, gun, s, staple, since, first, released, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will keep this brief: this is simply one of ...</td>\n",
       "      <td>/title/tt0060196/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film tells the story of three men who purs...</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>[keep, brief:, simply, one, entertaining, best...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review              Title  \\\n",
       "0  the conclusion to the series hits some of the ...  /title/tt1201607/   \n",
       "1  the lion king was pretty much my favourite mov...  /title/tt0110357/   \n",
       "2  ok, perhaps there is truth in the saying \"lost...  /title/tt0364569/   \n",
       "3  top gun has been an 's staple since it first r...  /title/tt1745960/   \n",
       "4  i will keep this brief: this is simply one of ...  /title/tt0060196/   \n",
       "\n",
       "   Spoiler_flag                                           Synopsis  \\\n",
       "0             0  After burying Dobby at the garden of the Shell...   \n",
       "1             1  The Lion King takes place in the Pride Lands o...   \n",
       "2             0  The film begins in medias res, with the silhou...   \n",
       "3             1  Over three decades after his time at TOPGUN, C...   \n",
       "4             0  The film tells the story of three men who purs...   \n",
       "\n",
       "   Cosine_Similarity  doc_similarity  \\\n",
       "0           0.158318        0.106248   \n",
       "1           0.197134        0.404854   \n",
       "2           0.115011        0.219557   \n",
       "3           0.423765        0.262304   \n",
       "4           0.084172        0.156379   \n",
       "\n",
       "                                              tokens  \n",
       "0  [conclusion, series, hits, strongest, emotiona...  \n",
       "1  [lion, king, pretty, much, favourite, movie, g...  \n",
       "2  [ok,, perhaps, truth, saying, lost, translatio...  \n",
       "3  [top, gun, s, staple, since, first, released, ...  \n",
       "4  [keep, brief:, simply, one, entertaining, best...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "506ab5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review               0\n",
       "Title                0\n",
       "Spoiler_flag         0\n",
       "Synopsis             0\n",
       "Cosine_Similarity    0\n",
       "doc_similarity       0\n",
       "tokens               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e0002c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Cleaned_Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cad0d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9548ecfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        the conclusion to the series hits some of the ...\n",
       "1        the lion king was pretty much my favourite mov...\n",
       "2        ok, perhaps there is truth in the saying \"lost...\n",
       "3        top gun has been an 's staple since it first r...\n",
       "4        i will keep this brief: this is simply one of ...\n",
       "                               ...                        \n",
       "22874    let us be honest: is this film really worthy o...\n",
       "22875    paul edgecomb (tom hanks) is the lead guard on...\n",
       "22876    this is undoubtedly the greatest film ever. th...\n",
       "22877    no, this is not an horrible movie and i did no...\n",
       "22878    i had heard so much of this film, and how posi...\n",
       "Name: Review, Length: 22879, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1df9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Spoiler_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a3b9147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "22874    0\n",
       "22875    0\n",
       "22876    0\n",
       "22877    0\n",
       "22878    1\n",
       "Name: Spoiler_flag, Length: 22879, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "655fe69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9effbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_val, y_train_2, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab616d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285      i have yet to watch a good or a very good mart...\n",
       "14544    film-making in the s was a different exercise ...\n",
       "13400    it is because of this movie that i began liste...\n",
       "6791     marvel seems to be on a roll towards killing o...\n",
       "1210     i have heard good things around this movie. so...\n",
       "                               ...                        \n",
       "11034    what can be said about this film that has not ...\n",
       "9141     great film. however, does not anyone else noti...\n",
       "22403    my next movie review is going to be on the hig...\n",
       "17543    guy richie's follow up to lock stock and two s...\n",
       "1296     set in during the height of world war i, this ...\n",
       "Name: Review, Length: 16472, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66aca6a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b174f82",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78c3b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "tv = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "cv_train_features = cv.fit_transform(X_train_2)\n",
    "tf_train_features = tv.fit_transform(X_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3eb82b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16472x56160 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1854562 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fbea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorizing test data\n",
    "cv_test_features = cv.transform(X_val)\n",
    "tf_test_features = tv.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b86fe309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn_pandas\n",
      "  Downloading sklearn_pandas-2.2.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=0.23.0 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.5.1 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from sklearn_pandas) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->sklearn_pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.1.4->sklearn_pandas) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\noahr\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23.0->sklearn_pandas) (1.1.0)\n",
      "Installing collected packages: sklearn-pandas\n",
      "Successfully installed sklearn-pandas-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\noahr\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn_pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "49125659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "26c68f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf_1 = ColumnTransformer([('Vect',CountVectorizer(stop_words='english'),['Review'])],remainder=\"passthrough\")\n",
    "trf_2 = LogisticRegression(max_iter=100,C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a2bc88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('sect_1',trf_1),('sect_2',trf_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "37d7ac81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;sect_1&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Vect&#x27;,\n",
       "                                                  CountVectorizer(stop_words=&#x27;english&#x27;),\n",
       "                                                  [&#x27;Review&#x27;])])),\n",
       "                (&#x27;sect_2&#x27;, LogisticRegression(C=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;sect_1&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Vect&#x27;,\n",
       "                                                  CountVectorizer(stop_words=&#x27;english&#x27;),\n",
       "                                                  [&#x27;Review&#x27;])])),\n",
       "                (&#x27;sect_2&#x27;, LogisticRegression(C=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">sect_1: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Vect&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;),\n",
       "                                 [&#x27;Review&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Vect</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Review&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre></pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('sect_1',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Vect',\n",
       "                                                  CountVectorizer(stop_words='english'),\n",
       "                                                  ['Review'])])),\n",
       "                ('sect_2', LogisticRegression(C=1))])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bb1fe0d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19356/2854864398.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Review\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spoiler_flag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    337\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    355\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_safe_indexing_column\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[1;32m--> 372\u001b[1;33m     \u001b[0mn_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0mkey_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_determine_key_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "pipe.fit(df[\"Review\"].iloc[0:200],df['Spoiler_flag'].iloc[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cc588ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5042)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26c5e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=100,C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5f8ba4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(a,df['Spoiler_flag'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "80b3da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review            how can i be disappointed by a good movie? i s...\n",
       "doc_similarity                                             0.140523\n",
       "Name: 101, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Review\",'doc_similarity']].iloc[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b656c625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahr\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "b = mapper.transform(df[[\"Review\",'doc_similarity']].iloc[104:106])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ffb37f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104    1\n",
       "105    0\n",
       "Name: Spoiler_flag, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Spoiler_flag\"].iloc[104:106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f9571f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a549a",
   "metadata": {},
   "source": [
    "# Model Implementation on Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0959169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=100,C=1)\n",
    "sgd = SGDClassifier(loss='log_loss',max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20659ebb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 22879]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19356/1981423430.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Spoiler_flag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1136\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1139\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 22879]"
     ]
    }
   ],
   "source": [
    "lr.fit(a,df['Spoiler_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da43c77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahr\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(cv_train_features,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc0438b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', max_iter=100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(cv_train_features,y_train_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2b5d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lr_cv = lr.predict(cv_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86579bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_svm_cv = sgd.predict(cv_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15fa61b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Logistic Regression : 0.6683661082787085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71      2299\n",
      "           1       0.63      0.60      0.61      1820\n",
      "\n",
      "    accuracy                           0.67      4119\n",
      "   macro avg       0.66      0.66      0.66      4119\n",
      "weighted avg       0.67      0.67      0.67      4119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy with Logistic Regression :', accuracy_score(Y_pred_lr_cv,y_val))\n",
    "print(classification_report(y_val,Y_pred_lr_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8202104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1666  633]\n",
      " [ 733 1087]]\n",
      "False negative ratio:  0.3188342757720748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, Y_pred_lr_cv)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Extract the false negatives count from the confusion matrix\n",
    "false_negatives = cm[1, 0]\n",
    "\n",
    "# Calculate the false negative ratio\n",
    "false_neg_ratio = false_negatives / sum(cm[0])\n",
    "\n",
    "# Print the false negative ratio\n",
    "print(\"False negative ratio: \", false_neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c755ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with SGD Regression : 0.6695799951444525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71      2299\n",
      "           1       0.64      0.58      0.61      1820\n",
      "\n",
      "    accuracy                           0.67      4119\n",
      "   macro avg       0.66      0.66      0.66      4119\n",
      "weighted avg       0.67      0.67      0.67      4119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy with SGD Regression :', accuracy_score(y_val,Y_pred_svm_cv))\n",
    "print(classification_report(y_val,Y_pred_svm_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "685df124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1696  603]\n",
      " [ 758 1062]]\n",
      "False negative ratio:  0.3297085689430187\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, Y_pred_svm_cv)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Extract the false negatives count from the confusion matrix\n",
    "false_negatives = cm[1, 0]\n",
    "\n",
    "# Calculate the false negative ratio\n",
    "false_neg_ratio = false_negatives / sum(cm[0])\n",
    "\n",
    "# Print the false negative ratio\n",
    "print(\"False negative ratio: \", false_neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "291d185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = \"For me, bringing back both the Grace character and the evil colonel felt too much. One of them, Grace (Sigorney Weaver), would have been sufficient, the Colonel could easily have been a brother like in Die Hard 3, although this would require additional story changes regarding the adolescent « Spider » character. But that would have been better than the whole « importance of Family » theme running through everything. Just too much of that for me, as well as let's make sure only incidental characters die.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3f06e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review = clean(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e17e1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectotized_sample = cv.transform([sample_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a06f37ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(vectotized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d91df64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2037932 , -1.69081623]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_log_proba(vectotized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4c8f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "\n",
    "model_param = {}\n",
    "model_param['coef'] = lr.coef_.tolist()\n",
    "model_param['intercepts'] = lr.intercept_.tolist()\n",
    "file_path = './lr.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43b72b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coef': [[-0.02561629398916542,\n",
       "   0.14651746597310253,\n",
       "   -0.04741759781525748,\n",
       "   0.04072336878105915,\n",
       "   0.03390795369918199,\n",
       "   0.03390795369918199,\n",
       "   -0.05438687118703611,\n",
       "   0.02643549023262049,\n",
       "   -0.09126927467976502,\n",
       "   0.023084118798052604,\n",
       "   -0.0058814939571116736,\n",
       "   0.02530200923918683,\n",
       "   -0.05438687118703611,\n",
       "   -0.05438687118703611,\n",
       "   0.08855703233715566,\n",
       "   -0.05438687118703611,\n",
       "   0.08522276567851586,\n",
       "   -0.12963969068678868,\n",
       "   0.17011913325285882,\n",
       "   0.016000258185050197,\n",
       "   -0.0418069893124803,\n",
       "   -0.04741759781525748,\n",
       "   -0.017700580558262013,\n",
       "   -0.020976760496869404,\n",
       "   -0.19721301258323198,\n",
       "   0.016000258185050197,\n",
       "   -0.05438687118703611,\n",
       "   -0.0058814939571116736,\n",
       "   0.013516463987530883,\n",
       "   -0.0418069893124803,\n",
       "   -0.0058814939571116736,\n",
       "   -0.0418069893124803,\n",
       "   -0.0418069893124803,\n",
       "   -0.05438687118703611,\n",
       "   -0.020976760496869404,\n",
       "   0.016000258185050197,\n",
       "   -0.017700580558262013,\n",
       "   -0.40986011306013015,\n",
       "   0.08522276567851586,\n",
       "   -0.04117045769978247,\n",
       "   -0.1021397893340924,\n",
       "   -0.06055213452273028,\n",
       "   0.24201060064804675,\n",
       "   -0.09089991662058154,\n",
       "   0.09589677197098476,\n",
       "   0.032814354467240126,\n",
       "   0.06613402508163524,\n",
       "   -0.024442726059468,\n",
       "   0.0008257619667695564,\n",
       "   0.05008792288985163,\n",
       "   0.1058189118588808,\n",
       "   0.04944044346351644,\n",
       "   -0.02284902964008044,\n",
       "   -0.2987026743764992,\n",
       "   -0.11355067912041379,\n",
       "   -0.0979831564214362,\n",
       "   0.05383533576496974,\n",
       "   0.03258425925300212,\n",
       "   0.004525320438071458,\n",
       "   0.019947077748846388,\n",
       "   -0.44068562656869587,\n",
       "   0.08513662965722939,\n",
       "   0.011316990011473925,\n",
       "   0.0011918715448128365,\n",
       "   -0.04970083318216852,\n",
       "   -0.1516999764306434,\n",
       "   -0.13990313768300353,\n",
       "   0.41211560472242,\n",
       "   0.015085735001019708,\n",
       "   0.041987460435470005,\n",
       "   0.3145441972746168,\n",
       "   -0.41145264102693,\n",
       "   0.11491281360216174,\n",
       "   0.3087392896049949,\n",
       "   0.0019911629737060397,\n",
       "   -0.06916021234413468,\n",
       "   0.0006768768615169616,\n",
       "   -0.05094428791283978,\n",
       "   0.0011193426623236566,\n",
       "   0.01953140787660734,\n",
       "   -0.0956042077130152,\n",
       "   0.007299594721152433,\n",
       "   0.0355899428183197,\n",
       "   0.09569078809939359,\n",
       "   -0.010350358807185923,\n",
       "   0.04793225480180647,\n",
       "   0.002178263111249862,\n",
       "   -0.2434046383429476,\n",
       "   -0.09825844647096822,\n",
       "   0.17060549235110647,\n",
       "   0.20092109266035146,\n",
       "   0.11204988045535064,\n",
       "   -0.021624131707847414,\n",
       "   -0.04288104491488894,\n",
       "   -0.14071776772557704,\n",
       "   0.024248459035984105,\n",
       "   0.03222626094307512,\n",
       "   0.00398772530101989,\n",
       "   -0.017333457481171575,\n",
       "   -0.08857884999380593,\n",
       "   0.07774173795397822,\n",
       "   0.003141404128744213,\n",
       "   -0.0364171882920782,\n",
       "   0.016002710519616364,\n",
       "   0.09103556657415464,\n",
       "   -0.08885217010242853,\n",
       "   -0.20503347652968829,\n",
       "   0.3270155368299704,\n",
       "   0.002627596812549677,\n",
       "   0.01585428457805695,\n",
       "   0.007965886096417851,\n",
       "   0.06981115201030373,\n",
       "   0.1324501447492389,\n",
       "   -0.22299666833788875,\n",
       "   0.026746044351036818,\n",
       "   0.0008977901293535567,\n",
       "   0.060297927088979436,\n",
       "   -0.06207474895398281,\n",
       "   -0.11161673797789219,\n",
       "   0.3728505401168276,\n",
       "   0.021319338102770258,\n",
       "   0.08415988728377022,\n",
       "   0.13311530141736833,\n",
       "   -0.09987796464444368,\n",
       "   -0.05006853168231025,\n",
       "   0.09589677197098476,\n",
       "   -0.012551135396855331,\n",
       "   0.0833812894506726,\n",
       "   0.1050442768842466,\n",
       "   -0.11229635576179525,\n",
       "   0.08853197967785648,\n",
       "   0.04600448488290353,\n",
       "   0.1643218258626888,\n",
       "   0.44978928197923285,\n",
       "   -0.1574357310100721,\n",
       "   0.0748811428910192,\n",
       "   -0.0486035811690108,\n",
       "   0.06924732169876825,\n",
       "   -0.6394798869682687,\n",
       "   0.02112605277980177,\n",
       "   0.0025058007016215463,\n",
       "   0.0007818926502481884,\n",
       "   0.061618598002393624,\n",
       "   0.03468327941709202,\n",
       "   -0.28990441906074227,\n",
       "   0.05797961727566924,\n",
       "   -0.16446920117264907,\n",
       "   0.35105198228948487,\n",
       "   -0.022700135500861397,\n",
       "   0.0028258932959209383,\n",
       "   0.1729825024909237,\n",
       "   0.24165426515798574,\n",
       "   0.48047954011922894,\n",
       "   0.042778137453034486,\n",
       "   -0.22327276150968184,\n",
       "   -0.09594992736620733,\n",
       "   0.011447720143984305,\n",
       "   -0.06031027397898962,\n",
       "   0.009998564709678331,\n",
       "   0.015092358179028306,\n",
       "   -0.10982498959822352,\n",
       "   0.2808422377856203,\n",
       "   0.06055432227708773,\n",
       "   0.5122991539694849,\n",
       "   -0.07502347529820397,\n",
       "   -0.16645340471422912,\n",
       "   0.010204498046207685,\n",
       "   -0.3040699424984226,\n",
       "   0.0020952470687029025,\n",
       "   0.19425413405713704,\n",
       "   0.1322967950130348,\n",
       "   0.003990646490689407,\n",
       "   -0.05889888653078631,\n",
       "   0.25578108507038444,\n",
       "   -0.008223879049437322,\n",
       "   0.12473470380878802,\n",
       "   -0.33929374142367336,\n",
       "   0.024428715417520487,\n",
       "   0.009790123018504397,\n",
       "   0.0036187164824465624,\n",
       "   0.012352710429636263,\n",
       "   0.17759584249517263,\n",
       "   -0.022678818340558734,\n",
       "   0.008328085554886115,\n",
       "   -0.23700477987886187,\n",
       "   0.11392067820851166,\n",
       "   0.005281816131152113,\n",
       "   0.012603333887078141,\n",
       "   -0.07513508464687182,\n",
       "   -0.5994178059216094,\n",
       "   0.3022647073469002,\n",
       "   0.030864266854648714,\n",
       "   -0.10706050135674437,\n",
       "   0.013957119571226055,\n",
       "   0.012682478528394884,\n",
       "   0.0368202329759524,\n",
       "   0.005114914433762705,\n",
       "   -0.21392833213025383,\n",
       "   -0.02284902964008044,\n",
       "   -0.03994493305774076,\n",
       "   0.25630840413880596,\n",
       "   -0.031058534683573162,\n",
       "   0.013531931080841985,\n",
       "   -0.05662066951557752,\n",
       "   -0.022191114796718925,\n",
       "   0.09619306864156557,\n",
       "   -0.034990575281884616,\n",
       "   0.5222521606592303,\n",
       "   -0.10071011245595278,\n",
       "   -0.0998258676141244,\n",
       "   -0.21751903102723974,\n",
       "   0.015404376271529085,\n",
       "   0.35943559850811413,\n",
       "   0.3670196555512041,\n",
       "   -0.18475108318065497,\n",
       "   0.08480215565873662,\n",
       "   0.03804494762888014,\n",
       "   -0.18036089477016268,\n",
       "   0.3576144088638264,\n",
       "   0.22499633828127755,\n",
       "   -0.044339773465673644,\n",
       "   -0.11429244444690058,\n",
       "   -0.04891546333798153,\n",
       "   -0.06339142215607953,\n",
       "   -0.06890614573090531,\n",
       "   -0.011117010689996487,\n",
       "   0.004296787292743398,\n",
       "   0.002402302940287718,\n",
       "   0.007545867253453554,\n",
       "   -0.16277500858067764,\n",
       "   -0.10172598873217639,\n",
       "   -0.10392605000640813,\n",
       "   -0.11035062263900965,\n",
       "   0.3228862885215559,\n",
       "   -0.05700742470086155,\n",
       "   0.00526614686287887,\n",
       "   0.08295193435481257,\n",
       "   -0.0443626304698496,\n",
       "   -0.0356389522543701,\n",
       "   0.10129090450235251,\n",
       "   0.441627406271296,\n",
       "   -0.007145282600878795,\n",
       "   0.2184833040467997,\n",
       "   0.08447026583506131,\n",
       "   0.07666307152603487,\n",
       "   -0.001684940912238493,\n",
       "   0.03598849512193811,\n",
       "   -0.28051002764363486,\n",
       "   0.007965886096417851,\n",
       "   -0.2574917946832854,\n",
       "   0.05985491475579973,\n",
       "   0.2601653941857074,\n",
       "   -0.20512968041924087,\n",
       "   0.12145450190000336,\n",
       "   -0.1543727333380615,\n",
       "   -0.07209208903429795,\n",
       "   -0.006685066976922647,\n",
       "   0.2058257329033818,\n",
       "   -0.018279228625607227,\n",
       "   -0.06405639337945905,\n",
       "   -0.01758268377238216,\n",
       "   0.0017767149583343968,\n",
       "   -0.20856310758955143,\n",
       "   0.019891101245377917,\n",
       "   -0.228768450025009,\n",
       "   -0.05629049720218975,\n",
       "   0.003256862961926515,\n",
       "   -0.037891418021544464,\n",
       "   -0.1765135268966,\n",
       "   -0.048790951844169324,\n",
       "   -0.29266230871854704,\n",
       "   0.4333736941722978,\n",
       "   0.4588496295636098,\n",
       "   0.005989631118080924,\n",
       "   -0.38717805827117735,\n",
       "   0.1295080417683856,\n",
       "   -0.2534175107729183,\n",
       "   0.12289382896193332,\n",
       "   -0.018900813852501714,\n",
       "   0.01730177909666755,\n",
       "   0.011354231470218353,\n",
       "   0.003176652324245238,\n",
       "   -0.04318194411759357,\n",
       "   0.016421908652022563,\n",
       "   0.14961619909482188,\n",
       "   -0.40193051017500386,\n",
       "   0.100906137395093,\n",
       "   -0.06428067828000521,\n",
       "   0.0072643986179111155,\n",
       "   0.3322502848270574,\n",
       "   -0.2947675530436905,\n",
       "   0.25654533022865983,\n",
       "   0.14522288983328258,\n",
       "   -0.01022405325685682,\n",
       "   -0.44470426745371827,\n",
       "   -0.014936150316564239,\n",
       "   -0.011000921970165256,\n",
       "   -0.09999513189625298,\n",
       "   0.006577954206787773,\n",
       "   -0.370953359822643,\n",
       "   0.4466743956238095,\n",
       "   -0.06099001669511401,\n",
       "   0.045650581990082637,\n",
       "   -0.2564434559961779,\n",
       "   0.42325892374832225,\n",
       "   0.17278332039986388,\n",
       "   0.13901526155911745,\n",
       "   -0.07224087331714774,\n",
       "   -0.14212908745829805,\n",
       "   -0.11647824474674527,\n",
       "   0.40084254511744816,\n",
       "   -0.15165658254320746,\n",
       "   -0.07385134762633006,\n",
       "   0.002246918155353574,\n",
       "   -0.11703437324381154,\n",
       "   0.11285618733122585,\n",
       "   0.8109331762483772,\n",
       "   -0.19121676905431012,\n",
       "   0.13672851105868822,\n",
       "   0.007135048607330162,\n",
       "   -0.7072502709322153,\n",
       "   -0.010339845611625577,\n",
       "   0.008114780513188298,\n",
       "   0.05566926786356627,\n",
       "   0.18571290423976994,\n",
       "   -0.06502162447962397,\n",
       "   0.0006938775800378772,\n",
       "   -0.020319244711056544,\n",
       "   0.04981042752402563,\n",
       "   0.15265966573123835,\n",
       "   -0.057366842853736236,\n",
       "   0.1915883151342482,\n",
       "   -0.016830732611819745,\n",
       "   0.012108113486686828,\n",
       "   0.04053575882603654,\n",
       "   -0.20373978582267974,\n",
       "   0.13208160361070292,\n",
       "   0.018633031867010465,\n",
       "   -0.13276078847009137,\n",
       "   0.26699063972675713,\n",
       "   0.4495873624109937,\n",
       "   0.004598675517844249,\n",
       "   0.4680853427776675,\n",
       "   -0.004862162911687196,\n",
       "   0.001305371224685957,\n",
       "   0.39587768344455143,\n",
       "   -0.21623910807852909,\n",
       "   -0.3667546095947268,\n",
       "   -0.13354104711786585,\n",
       "   -0.0836037885982073,\n",
       "   0.32134036221884094,\n",
       "   0.06102579812341744,\n",
       "   -0.8269900366253858,\n",
       "   0.14872794063989514,\n",
       "   0.23064816017131787,\n",
       "   -0.021439559333876097,\n",
       "   0.010182045957607909,\n",
       "   -0.4691111052345046,\n",
       "   -0.3721157333232265,\n",
       "   0.0558646116695921,\n",
       "   -0.08658587847578864,\n",
       "   -0.015920240857679548,\n",
       "   0.07867879613113817,\n",
       "   -0.02263838350171565,\n",
       "   -0.021272340571733717,\n",
       "   0.3680434522983991,\n",
       "   -0.39556146697431716,\n",
       "   -0.15761307198256336,\n",
       "   0.047389716889764476,\n",
       "   -0.050138759136412005,\n",
       "   -0.175964227967124,\n",
       "   0.09789308210652645,\n",
       "   0.3871908942927949,\n",
       "   -0.03462955764938626,\n",
       "   0.08832192308678251,\n",
       "   0.12236636307328132,\n",
       "   0.1561746116280804,\n",
       "   -0.019698501479048405,\n",
       "   -0.05042211268522037,\n",
       "   0.022517517433781897,\n",
       "   0.001389852776897421,\n",
       "   -0.010267745026801441,\n",
       "   -0.13110348090154034,\n",
       "   0.016117668024736787,\n",
       "   0.15083834408554458,\n",
       "   -0.11549944676750219,\n",
       "   0.03715494369243865,\n",
       "   0.6244448124255165,\n",
       "   0.8316803075001392,\n",
       "   0.09326335365617575,\n",
       "   0.3233299152043699,\n",
       "   0.024585669677890387,\n",
       "   0.05691558666099629,\n",
       "   0.007729440107800635,\n",
       "   -0.28715025207535455,\n",
       "   0.02477297924259222,\n",
       "   -0.15707327343756097,\n",
       "   0.11551691941004413,\n",
       "   -0.02627629303755114,\n",
       "   0.0339131285371757,\n",
       "   -0.09102894391636504,\n",
       "   -0.18198180587004606,\n",
       "   0.08903855353258511,\n",
       "   -0.12545081484151277,\n",
       "   0.44561882471112724,\n",
       "   -0.11224089429460661,\n",
       "   -0.17175141198236366,\n",
       "   0.4603313691924543,\n",
       "   0.03867427398991312,\n",
       "   -0.0571689599196022,\n",
       "   0.004088849242967485,\n",
       "   -0.04223264584652658,\n",
       "   0.1764260807862555,\n",
       "   0.04215545438872193,\n",
       "   -0.4032351780102531,\n",
       "   0.31304116443716473,\n",
       "   0.19267014390100612,\n",
       "   -0.0246937291485861,\n",
       "   0.5549560682762715,\n",
       "   -0.10695272177608219,\n",
       "   0.09368420173479533,\n",
       "   -0.7433266235396131,\n",
       "   -0.3493431111432701,\n",
       "   -0.2449099391803261,\n",
       "   -0.14005877508700135,\n",
       "   0.1501774988510892,\n",
       "   -0.05233387543024506,\n",
       "   -0.22179302932080425,\n",
       "   -0.01131908274431343,\n",
       "   0.08938315126475754,\n",
       "   0.14146724666081073,\n",
       "   -0.04489678339267624,\n",
       "   0.052083268272808514,\n",
       "   0.003951384778727808,\n",
       "   -0.030717970461608877,\n",
       "   0.061973889651365716,\n",
       "   -0.008455129000540474,\n",
       "   -0.18630573650281604,\n",
       "   0.027297432562912748,\n",
       "   0.1455269900017221,\n",
       "   -0.044372908804051384,\n",
       "   0.014452007923992679,\n",
       "   0.05315481794375053,\n",
       "   0.2399464166763777,\n",
       "   -0.030510126568635144,\n",
       "   0.005885597255179639,\n",
       "   -0.06940400120612614,\n",
       "   0.12693322417699993,\n",
       "   -0.007976899545657452,\n",
       "   0.005068640543706043,\n",
       "   0.09861149216523085,\n",
       "   0.0420019328362464,\n",
       "   0.030942336074571062,\n",
       "   -0.05023559563173081,\n",
       "   0.14675741571489903,\n",
       "   0.047330131237536176,\n",
       "   -0.11832711295460836,\n",
       "   0.003215883298339121,\n",
       "   0.02761693753260298,\n",
       "   -0.032933229072922066,\n",
       "   0.08724447390169686,\n",
       "   0.0013956060622457688,\n",
       "   -0.4278316050417688,\n",
       "   0.0007441247607307284,\n",
       "   0.0009519058216568834,\n",
       "   0.044490963265691935,\n",
       "   0.05350026255500613,\n",
       "   0.0017137927285622648,\n",
       "   -0.14210652803891188,\n",
       "   0.9485834176327886,\n",
       "   0.1292533315726565,\n",
       "   0.008027904255136798,\n",
       "   0.09167170574291965,\n",
       "   0.0036038427340087343,\n",
       "   0.11033757618255785,\n",
       "   -0.015478785858337951,\n",
       "   -0.026619805987393018,\n",
       "   0.0996404842929923,\n",
       "   0.13696772126218468,\n",
       "   -0.003070542012841854,\n",
       "   0.03693416241338219,\n",
       "   0.02753053804435892,\n",
       "   0.060135866850627016,\n",
       "   -0.2816170292039055,\n",
       "   0.12749754510141859,\n",
       "   0.07623406244726458,\n",
       "   0.002355327382847561,\n",
       "   -0.046244139121766364,\n",
       "   -0.072739463658004,\n",
       "   -0.08120599650128225,\n",
       "   -0.09649772920620991,\n",
       "   0.09027458498543,\n",
       "   -0.025305069453729324,\n",
       "   -0.024520793641618993,\n",
       "   0.03906815065176769,\n",
       "   0.05855642410186463,\n",
       "   0.17679627262229464,\n",
       "   -0.05104101295501144,\n",
       "   -0.034182021254309254,\n",
       "   -0.25369168622571514,\n",
       "   -0.25369168622571514,\n",
       "   0.3482941915036672,\n",
       "   0.02675774208743929,\n",
       "   0.06694875933602208,\n",
       "   0.051384223927408434,\n",
       "   0.06654183062486643,\n",
       "   -0.06738238657506955,\n",
       "   0.02162088320696725,\n",
       "   0.10048216826855991,\n",
       "   0.3377448111319315,\n",
       "   -0.18007752327350612,\n",
       "   0.06355314625716316,\n",
       "   -0.0693796018719171,\n",
       "   0.5349179391889115,\n",
       "   0.19715848692345686,\n",
       "   -0.012334337855700083,\n",
       "   0.05676827141931375,\n",
       "   0.04668525613358493,\n",
       "   -0.17923046410505517,\n",
       "   -0.40814534663938595,\n",
       "   -0.27043371752491163,\n",
       "   -0.02853093220486139,\n",
       "   -0.6837119555937522,\n",
       "   0.6708499037440033,\n",
       "   -0.014655156906524535,\n",
       "   0.013153498437654414,\n",
       "   0.01708848294228839,\n",
       "   -0.04708936037063726,\n",
       "   0.08105496809113644,\n",
       "   -0.004679060101372959,\n",
       "   0.11501273989677328,\n",
       "   -0.01653071610011662,\n",
       "   0.026664741280602994,\n",
       "   -0.043595575315756974,\n",
       "   0.026408333772609857,\n",
       "   -0.066243174167998,\n",
       "   0.3282491531141179,\n",
       "   0.8024991829276628,\n",
       "   -0.04720616793284415,\n",
       "   -0.10249615578474565,\n",
       "   0.09149184827153678,\n",
       "   -0.30090938090096375,\n",
       "   0.005041799791214104,\n",
       "   -0.1320768985968171,\n",
       "   -0.21728934493670962,\n",
       "   -0.046926528565738534,\n",
       "   0.01293226382147916,\n",
       "   0.49343848337546514,\n",
       "   -0.030295390583368935,\n",
       "   0.013554576943584997,\n",
       "   -0.605123815479753,\n",
       "   -0.5822138873601252,\n",
       "   -0.1688369616070739,\n",
       "   0.09029100978725982,\n",
       "   0.1354190315835732,\n",
       "   0.11500593132077727,\n",
       "   -0.21596752832517518,\n",
       "   0.07817356296381578,\n",
       "   0.0012316272947697157,\n",
       "   -0.04130163071040147,\n",
       "   0.0032605261962647208,\n",
       "   -0.023154672878115883,\n",
       "   0.17264187567312672,\n",
       "   0.3478605953313325,\n",
       "   0.08840315051052525,\n",
       "   0.2783965751583259,\n",
       "   0.0266897591096476,\n",
       "   -0.04661348640778631,\n",
       "   0.057506971344557054,\n",
       "   0.4830070708506447,\n",
       "   -0.6940216138044443,\n",
       "   -0.0663764099671737,\n",
       "   -0.18746457125007462,\n",
       "   -0.07836248350766445,\n",
       "   -0.19837129483579677,\n",
       "   0.012967284496240471,\n",
       "   0.15479479459805728,\n",
       "   0.0007616035051677837,\n",
       "   -0.1623272311938534,\n",
       "   0.5683797895898439,\n",
       "   0.20022168103818966,\n",
       "   -0.46047036567537863,\n",
       "   1.1792822597341424,\n",
       "   0.08859549223467074,\n",
       "   -0.5732647205778576,\n",
       "   0.24845497066839867,\n",
       "   -0.3466621380092942,\n",
       "   -0.49240389268392837,\n",
       "   0.3697202787335994,\n",
       "   -0.08141920481592048,\n",
       "   0.3532066693101263,\n",
       "   0.0013343048939003694,\n",
       "   -0.19884764863251658,\n",
       "   0.26416275263735495,\n",
       "   0.010864507101072764,\n",
       "   -0.027338736737345177,\n",
       "   0.058278031082268755,\n",
       "   -0.06567230596758962,\n",
       "   0.18626155043064618,\n",
       "   -0.0943324113868576,\n",
       "   0.20215877233527477,\n",
       "   -0.32193712079251985,\n",
       "   0.14529672760089793,\n",
       "   -0.14527671005113943,\n",
       "   0.0022840495844520655,\n",
       "   0.12615383717009238,\n",
       "   0.17145971496387233,\n",
       "   0.23664070134987303,\n",
       "   -0.15098473172955043,\n",
       "   0.042152997099438747,\n",
       "   0.033723883195248,\n",
       "   -0.1410934684558342,\n",
       "   0.008408948148867312,\n",
       "   -0.10932901756447362,\n",
       "   -0.01085257278700908,\n",
       "   0.0033238171303578115,\n",
       "   0.06901790621731077,\n",
       "   -0.1095141597961714,\n",
       "   -0.29145363867514895,\n",
       "   -0.5510690530796092,\n",
       "   0.13145969589870415,\n",
       "   -0.1796160351506125,\n",
       "   0.1489632145011981,\n",
       "   -0.0032313620911336998,\n",
       "   -0.08295298618593717,\n",
       "   -0.16004707335257382,\n",
       "   -0.031220138761227336,\n",
       "   -0.050512942724511,\n",
       "   0.0019748713616585985,\n",
       "   -0.1371602991912291,\n",
       "   0.1634053343033543,\n",
       "   -0.017765761898843487,\n",
       "   -0.0804374845272983,\n",
       "   -0.0845655361626864,\n",
       "   -0.16975192070812678,\n",
       "   -0.3395730049263303,\n",
       "   0.003014401741329179,\n",
       "   0.17987774523157,\n",
       "   -0.008725756400401748,\n",
       "   0.09215923786499688,\n",
       "   -0.36400833109533853,\n",
       "   -0.7245950898093102,\n",
       "   0.26040051578106826,\n",
       "   -0.0722447640595021,\n",
       "   -0.5054624743013179,\n",
       "   -0.9016947376236619,\n",
       "   0.6725966424019485,\n",
       "   -0.314312395197539,\n",
       "   -0.1427063523179706,\n",
       "   -0.49508269041594843,\n",
       "   0.23886885748370792,\n",
       "   -0.11635582079203406,\n",
       "   0.15551821265366386,\n",
       "   0.10465846574752603,\n",
       "   -0.020775453897441207,\n",
       "   -0.08914325701275523,\n",
       "   -0.15462490007332025,\n",
       "   0.1376793620485932,\n",
       "   0.03653249566539054,\n",
       "   0.058243812022469255,\n",
       "   0.6867950049179932,\n",
       "   -0.0318297916494832,\n",
       "   0.2536308846030318,\n",
       "   0.02162088320696725,\n",
       "   0.09394827002180586,\n",
       "   0.0026991129175022847,\n",
       "   0.012105057950065662,\n",
       "   -0.05712303799416218,\n",
       "   -0.1889751670248077,\n",
       "   0.17516941541422107,\n",
       "   0.001278542917576287,\n",
       "   -0.030755368273586534,\n",
       "   -0.4859322485650344,\n",
       "   0.676540203518811,\n",
       "   -0.22454156841859868,\n",
       "   -0.299501115227465,\n",
       "   0.1433657188046007,\n",
       "   -0.45668728342697046,\n",
       "   -0.14852360712703835,\n",
       "   0.15537709196400684,\n",
       "   -0.25646043717853695,\n",
       "   0.15589208120628895,\n",
       "   -0.3261239655513887,\n",
       "   0.03466654963065098,\n",
       "   0.06775201332542641,\n",
       "   -0.6571758824333561,\n",
       "   0.460566223948152,\n",
       "   0.28120516404215085,\n",
       "   0.09460891537918767,\n",
       "   -0.11505497450180517,\n",
       "   0.08668973531998307,\n",
       "   -0.06652426496397772,\n",
       "   -0.021624131707847414,\n",
       "   -0.08145958881700789,\n",
       "   0.17556978674046217,\n",
       "   0.7465480683494584,\n",
       "   -0.03953320513019167,\n",
       "   0.0238543159393419,\n",
       "   0.008782329615509827,\n",
       "   -0.012611501907268303,\n",
       "   0.24142792900978,\n",
       "   0.08386969961746325,\n",
       "   1.093475045707229,\n",
       "   -0.024598213560151027,\n",
       "   -0.14158079577185806,\n",
       "   0.08086790613440323,\n",
       "   0.09992059475635855,\n",
       "   0.05277543195690346,\n",
       "   -0.5465406943619254,\n",
       "   0.12236636307328132,\n",
       "   0.006610611744237237,\n",
       "   -0.2000852490671012,\n",
       "   0.17595646694622138,\n",
       "   0.43656011033697467,\n",
       "   -0.1297216358744485,\n",
       "   -0.013813789011713432,\n",
       "   0.05142146641021019,\n",
       "   -0.021157794248558275,\n",
       "   -0.04708936037063726,\n",
       "   -0.1031998089019178,\n",
       "   0.004628168391661191,\n",
       "   0.0009380724897682957,\n",
       "   0.02715145624790177,\n",
       "   -0.4008098982046786,\n",
       "   0.12391249972751514,\n",
       "   -0.20273814399273335,\n",
       "   -0.013259490049414796,\n",
       "   0.0033238171303578115,\n",
       "   0.12372388115221636,\n",
       "   0.03742485195368007,\n",
       "   0.028457717103312595,\n",
       "   0.1463716046361213,\n",
       "   0.14344192522704907,\n",
       "   0.05254616080577061,\n",
       "   -0.045384739340299546,\n",
       "   -0.48209423394549167,\n",
       "   0.17721968986945819,\n",
       "   -0.026841164691376963,\n",
       "   -0.24591445281332672,\n",
       "   -0.2703594067480489,\n",
       "   -0.13837645037739854,\n",
       "   -0.14604300170020862,\n",
       "   0.06586192383611761,\n",
       "   0.037382397386132096,\n",
       "   -0.14627386269859047,\n",
       "   0.012967284496240471,\n",
       "   -0.11507959239596732,\n",
       "   0.08378689668872731,\n",
       "   0.23190184681852954,\n",
       "   0.03051007520974947,\n",
       "   0.05718839685656521,\n",
       "   -0.6746779093119847,\n",
       "   -0.07455333490587855,\n",
       "   0.013961354655086165,\n",
       "   0.08844031941578331,\n",
       "   0.05471128681656199,\n",
       "   0.3200298184909633,\n",
       "   -0.31676088753154497,\n",
       "   -0.13548285953007197,\n",
       "   -0.11719489977329642,\n",
       "   0.06400359073939813,\n",
       "   0.49872814851321556,\n",
       "   0.4067359254022626,\n",
       "   0.007896961772491405,\n",
       "   0.277387490717423,\n",
       "   0.20961420369225764,\n",
       "   -0.1996000937984086,\n",
       "   0.0477798067624944,\n",
       "   0.042299669287078394,\n",
       "   -0.12624441315639442,\n",
       "   0.4569366085808925,\n",
       "   0.05254490873592874,\n",
       "   -0.1270000578934311,\n",
       "   -0.14294653989867062,\n",
       "   0.1541217176065294,\n",
       "   -0.17419078444529504,\n",
       "   0.012072421214047318,\n",
       "   -0.08272505657060318,\n",
       "   -0.030216593227602805,\n",
       "   0.22151084055298859,\n",
       "   0.0061458330998944285,\n",
       "   0.26370294760785257,\n",
       "   0.26363927030806705,\n",
       "   0.26785030989170705,\n",
       "   0.0694301939880069,\n",
       "   -0.15912793975943584,\n",
       "   0.026957436221482094,\n",
       "   0.19529816305411407,\n",
       "   0.03852213154747481,\n",
       "   0.0021357362920494034,\n",
       "   -0.01880485118545711,\n",
       "   0.058583548649000915,\n",
       "   0.12672280562482227,\n",
       "   -0.008340738861607949,\n",
       "   0.03785574865234479,\n",
       "   0.39987640715954376,\n",
       "   0.013591589198702311,\n",
       "   0.024428715417520487,\n",
       "   0.10220942028571743,\n",
       "   0.026352813334283902,\n",
       "   0.043241001924374745,\n",
       "   -0.05312152869350447,\n",
       "   0.006177055187808371,\n",
       "   -0.02665433529074335,\n",
       "   -0.016108720358455973,\n",
       "   0.07730319271596371,\n",
       "   0.13038545381008662,\n",
       "   0.007469069347335579,\n",
       "   -0.37013838947693917,\n",
       "   0.13337018888617544,\n",
       "   -0.19235394534386913,\n",
       "   -0.09208326639295107,\n",
       "   0.03971279601436329,\n",
       "   -0.07670470141828371,\n",
       "   -0.11734395056652669,\n",
       "   0.22435412421922962,\n",
       "   0.12422828268356388,\n",
       "   0.1273582887442713,\n",
       "   0.4174471334055875,\n",
       "   0.008877647415572181,\n",
       "   0.5233060305204632,\n",
       "   -0.12506365235405328,\n",
       "   -0.014168967243260878,\n",
       "   0.14559256872713577,\n",
       "   -0.3796671222839845,\n",
       "   -0.14484067486933175,\n",
       "   0.1186912150301716,\n",
       "   -0.04619014881580321,\n",
       "   0.061253893202297745,\n",
       "   0.2714536938894984,\n",
       "   -0.08853049480092032,\n",
       "   -0.04384486800273271,\n",
       "   -0.11149472639494085,\n",
       "   0.0666223211098423,\n",
       "   0.15719944631472477,\n",
       "   0.13678441326623886,\n",
       "   -0.04999190692307738,\n",
       "   -0.22391071201867163,\n",
       "   0.17641083909431352,\n",
       "   -0.07224087331714774,\n",
       "   -0.07728615075853702,\n",
       "   0.4424860329251359,\n",
       "   0.030929616029141647,\n",
       "   0.08350472584341563,\n",
       "   0.20217261478819268,\n",
       "   0.014171840769262173,\n",
       "   -0.04120299624296472,\n",
       "   -0.5539965019576063,\n",
       "   0.02713765837135299,\n",
       "   -0.006332307662343824,\n",
       "   -0.059132913265024664,\n",
       "   -0.14205516139511143,\n",
       "   -0.08806811507370321,\n",
       "   -0.3404814754762807,\n",
       "   0.8500574364375149,\n",
       "   -0.03431559830279778,\n",
       "   0.27126070290330784,\n",
       "   -0.19738814901812476,\n",
       "   0.05805473414770357,\n",
       "   0.09253325924421087,\n",
       "   -0.08814980544030084,\n",
       "   0.001747576078903045,\n",
       "   -0.048564575035671645,\n",
       "   -0.42269066930093707,\n",
       "   -0.4401997459161274,\n",
       "   -0.21252491152121755,\n",
       "   0.15148434923900117,\n",
       "   -0.2133548734534846,\n",
       "   0.031151085849843694,\n",
       "   -0.04092084859507197,\n",
       "   0.09846015063076816,\n",
       "   -0.020387976002771428,\n",
       "   -0.022594612019265388,\n",
       "   0.12193931078473058,\n",
       "   -0.6041907459266082,\n",
       "   0.09361537037408367,\n",
       "   0.16993090361784347,\n",
       "   -0.10210277172805429,\n",
       "   0.009652057002130138,\n",
       "   0.009652057002130138,\n",
       "   -0.29909016877036737,\n",
       "   -0.2290218680155301,\n",
       "   0.8677006732804107,\n",
       "   0.052801177668176276,\n",
       "   -0.06122242731863688,\n",
       "   0.15229903589397967,\n",
       "   0.16215802118049777,\n",
       "   -0.08167728463018352,\n",
       "   -0.5137488712051305,\n",
       "   0.5804759705587974,\n",
       "   -0.032140197614880094,\n",
       "   0.010747308902343666,\n",
       "   0.08410844781132547,\n",
       "   -0.007073462323077269,\n",
       "   0.01068618261821059,\n",
       "   -0.10905890773492391,\n",
       "   -0.18477047841375974,\n",
       "   0.10417806427910392,\n",
       "   0.1600754887257669,\n",
       "   -0.012846978706525325,\n",
       "   0.08886912128152583,\n",
       "   -0.2936603946735263,\n",
       "   0.10500168939658776,\n",
       "   -0.10422573738686226,\n",
       "   0.02257719912638315,\n",
       "   0.008915315245080529,\n",
       "   0.17165215919301482,\n",
       "   0.10991497224421576,\n",
       "   0.13782336284185287,\n",
       "   -0.5494836512230985,\n",
       "   -0.2671923094890651,\n",
       "   0.01651661749013569,\n",
       "   0.3189451908500003,\n",
       "   -0.03462311448782783,\n",
       "   -0.012737505760544521,\n",
       "   0.34247634741852506,\n",
       "   -0.8336889806445503,\n",
       "   -0.3237537831902539,\n",
       "   0.013448047796371475,\n",
       "   0.10966096617298685,\n",
       "   0.04957197510592364,\n",
       "   0.13473222874949484,\n",
       "   0.006123573918593132,\n",
       "   -0.38649080401775815,\n",
       "   -0.056194433638478626,\n",
       "   0.07922095936630949,\n",
       "   0.04904933132179685,\n",
       "   0.0029365879979820026,\n",
       "   0.0021357362920494034,\n",
       "   -0.12065544596231571,\n",
       "   -0.009413375084008378,\n",
       "   -0.018429832911101847,\n",
       "   -0.33358646119549945,\n",
       "   0.008970940901041856,\n",
       "   -0.17713103263560298,\n",
       "   -0.025071313209630845,\n",
       "   -0.060452366511973624,\n",
       "   0.04906255109113877,\n",
       "   0.0036038427340087343,\n",
       "   0.015404376271529085,\n",
       "   -0.01889056361964858,\n",
       "   0.04806475825336594,\n",
       "   0.23096473979100837,\n",
       "   -0.09646257419349473,\n",
       "   0.13427293209455887,\n",
       "   0.0012169363748734871,\n",
       "   -0.06501830345557745,\n",
       "   0.020265225821411208,\n",
       "   0.23166229188955786,\n",
       "   -0.0574704383849283,\n",
       "   -0.1314670540477954,\n",
       "   0.02064622406658,\n",
       "   0.09435687306631074,\n",
       "   0.1548075976197576,\n",
       "   0.24721196428781095,\n",
       "   -0.028165109870511407,\n",
       "   0.15699051324754476,\n",
       "   0.02508746579361397,\n",
       "   -0.042408616921836784,\n",
       "   -0.18490260323613092,\n",
       "   0.009079861170355539,\n",
       "   -0.01350147429290557,\n",
       "   0.19731923217066266,\n",
       "   -0.047576814071727536,\n",
       "   0.0016527376105799879,\n",
       "   0.051794264487632385,\n",
       "   0.2414809222371262,\n",
       "   -0.0932416573771158,\n",
       "   0.2559362725037065,\n",
       "   0.004284574266203011,\n",
       "   0.009998564709678331,\n",
       "   0.00824906969932052,\n",
       "   -0.09316706375585304,\n",
       "   0.05607026211306298,\n",
       "   0.06475685429040456,\n",
       "   -0.25754650885769725,\n",
       "   -0.0855078556463022,\n",
       "   -0.18404232524577274,\n",
       "   0.0033062348522403797,\n",
       "   0.06247385231781255,\n",
       "   0.1475783510313855,\n",
       "   -0.11950940848117819,\n",
       "   0.030282012353754453,\n",
       "   0.33095287793324013,\n",
       "   0.2649428257733714,\n",
       "   0.32863406293191694,\n",
       "   -0.008238552819739077,\n",
       "   -0.021443357932476075,\n",
       "   0.035601695270137824,\n",
       "   0.016189731476245882,\n",
       "   0.018711437216619282,\n",
       "   -0.01859228011771865,\n",
       "   -0.4489422306270722,\n",
       "   0.20410996584538146,\n",
       "   -0.2350292701620272,\n",
       "   0.008883441384705329,\n",
       "   -0.004663966491069236,\n",
       "   -0.1706038432358339,\n",
       "   -0.358813854016989,\n",
       "   0.09496973357340593,\n",
       "   ...]],\n",
       " 'intercepts': [-1.8803756775200064]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20335a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(model_param, codecs.open(file_path, 'w', encoding='utf-8'), \n",
    "          separators=(',', ':'), \n",
    "          sort_keys=True, \n",
    "          indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b7d1fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2176/3120227593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lr.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json_text' is not defined"
     ]
    }
   ],
   "source": [
    "with open('lr.json','w') as file:\n",
    "    file.write(json_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e623b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def logistic_regression_to_json(lrmodel, file=None):\n",
    "    if file is not None:\n",
    "        serialize = lambda x: json.dump(x, file)\n",
    "    else:\n",
    "        serialize = json.dumps\n",
    "    data = {}\n",
    "    data['init_params'] = lrmodel.get_params()\n",
    "    data['model_params'] = mp = {}\n",
    "    for p in ('coef_', 'intercept_','classes_', 'n_iter_'):\n",
    "        mp[p] = getattr(lrmodel, p).tolist()\n",
    "    return serialize(data)\n",
    "\n",
    "def logistic_regression_from_json(jstring):\n",
    "    data = json.loads(jstring)\n",
    "    model = LogisticRegression(**data['init_params'])\n",
    "    for name, p in data['model_params'].items():\n",
    "        setattr(model, name, np.array(p))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d44d0ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_to_json(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "467c3193",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9531e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "168dd657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(cv_train_features,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7edbb4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_rf_cv = rf_clf.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e19a314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1750  549]\n",
      " [ 780 1040]]\n",
      "False negative ratio:  0.3392779469334493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, Y_pred_rf_cv)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Extract the false negatives count from the confusion matrix\n",
    "false_negatives = cm[1, 0]\n",
    "\n",
    "# Calculate the false negative ratio\n",
    "false_neg_ratio = false_negatives / sum(cm[0])\n",
    "\n",
    "# Print the false negative ratio\n",
    "print(\"False negative ratio: \", false_neg_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71bafa",
   "metadata": {},
   "source": [
    "# Model Implementation on Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "10733be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=100,C=1)\n",
    "sgd = SGDClassifier(loss='log_loss',max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3f42e38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(tf_train_features,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "abc6ff57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', max_iter=100)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.fit(tf_train_features,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "be02cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lr_tf = lr.predict(tf_test_features)\n",
    "Y_pred_svm_tf = sgd.predict(tf_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "baff6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Logistic Regression : 0.7137654770575382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.75      2299\n",
      "           1       0.69      0.63      0.66      1820\n",
      "\n",
      "    accuracy                           0.71      4119\n",
      "   macro avg       0.71      0.71      0.71      4119\n",
      "weighted avg       0.71      0.71      0.71      4119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy with Logistic Regression :', accuracy_score(Y_pred_lr_tf,y_val))\n",
    "print(classification_report(y_val,Y_pred_lr_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d88bcabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1792  507]\n",
      " [ 672 1148]]\n",
      "False negative ratio:  0.2923010004349717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, Y_pred_lr_tf)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Extract the false negatives count from the confusion matrix\n",
    "false_negatives = cm[1, 0]\n",
    "\n",
    "# Calculate the false negative ratio\n",
    "false_neg_ratio = false_negatives / sum(cm[0])\n",
    "\n",
    "# Print the false negative ratio\n",
    "print(\"False negative ratio: \", false_neg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d3f3b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with SGD Regression : 0.714008254430687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.75      2299\n",
      "           1       0.70      0.63      0.66      1820\n",
      "\n",
      "    accuracy                           0.71      4119\n",
      "   macro avg       0.71      0.71      0.71      4119\n",
      "weighted avg       0.71      0.71      0.71      4119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy with SGD Regression :', accuracy_score(y_val,Y_pred_svm_tf))\n",
    "print(classification_report(y_val,Y_pred_svm_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9b8bba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1798  501]\n",
      " [ 677 1143]]\n",
      "False negative ratio:  0.2944758590691605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, Y_pred_svm_tf)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "# Extract the false negatives count from the confusion matrix\n",
    "false_negatives = cm[1, 0]\n",
    "\n",
    "# Calculate the false negative ratio\n",
    "false_neg_ratio = false_negatives / sum(cm[0])\n",
    "\n",
    "# Print the false negative ratio\n",
    "print(\"False negative ratio: \", false_neg_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d702f6",
   "metadata": {},
   "source": [
    "# Model Implementation with similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dbfc6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "39729d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=100,C=1)\n",
    "sgd = SGDClassifier(loss='log_loss',max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9b747581",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['doc_similarity','Review']]\n",
    "y= df['Spoiler_flag']\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7344c945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>doc_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13209</th>\n",
       "      <td>every time i watch an iranian film i am amazed...</td>\n",
       "      <td>0.253923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13813</th>\n",
       "      <td>dom de luise once uttered a line i use quite o...</td>\n",
       "      <td>0.118664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6938</th>\n",
       "      <td>it is a great movie! charles chaplin is able t...</td>\n",
       "      <td>0.327167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9289</th>\n",
       "      <td>no need to recap plot or consensus points. in ...</td>\n",
       "      <td>0.064079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15753</th>\n",
       "      <td>tgbh is one of my favourite films ever. it is ...</td>\n",
       "      <td>0.306137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>two other titles came to mind as i watched thi...</td>\n",
       "      <td>0.192651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>in , in chicago, the musicians and friends joe...</td>\n",
       "      <td>0.367849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>this adventure flick, which mixes the western ...</td>\n",
       "      <td>0.177938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12172</th>\n",
       "      <td>\"there can be no understanding between the han...</td>\n",
       "      <td>0.283252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>early on this movie establishes that \"whiplash...</td>\n",
       "      <td>0.392834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20591 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  doc_similarity\n",
       "13209  every time i watch an iranian film i am amazed...        0.253923\n",
       "13813  dom de luise once uttered a line i use quite o...        0.118664\n",
       "6938   it is a great movie! charles chaplin is able t...        0.327167\n",
       "9289   no need to recap plot or consensus points. in ...        0.064079\n",
       "15753  tgbh is one of my favourite films ever. it is ...        0.306137\n",
       "...                                                  ...             ...\n",
       "10955  two other titles came to mind as i watched thi...        0.192651\n",
       "17289  in , in chicago, the musicians and friends joe...        0.367849\n",
       "5192   this adventure flick, which mixes the western ...        0.177938\n",
       "12172  \"there can be no understanding between the han...        0.283252\n",
       "235    early on this movie establishes that \"whiplash...        0.392834\n",
       "\n",
       "[20591 rows x 2 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[['Review','doc_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "53eded1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13209    0.253923\n",
       "13813    0.118664\n",
       "6938     0.327167\n",
       "9289     0.064079\n",
       "15753    0.306137\n",
       "           ...   \n",
       "10955    0.192651\n",
       "17289    0.367849\n",
       "5192     0.177938\n",
       "12172    0.283252\n",
       "235      0.392834\n",
       "Name: doc_similarity, Length: 20591, dtype: float64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['doc_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8eb1a600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Title</th>\n",
       "      <th>Spoiler_flag</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>doc_similarity</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the conclusion to the series hits some of the ...</td>\n",
       "      <td>/title/tt1201607/</td>\n",
       "      <td>0</td>\n",
       "      <td>After burying Dobby at the garden of the Shell...</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.106248</td>\n",
       "      <td>[conclusion, series, hits, strongest, emotiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the lion king was pretty much my favourite mov...</td>\n",
       "      <td>/title/tt0110357/</td>\n",
       "      <td>1</td>\n",
       "      <td>The Lion King takes place in the Pride Lands o...</td>\n",
       "      <td>0.197134</td>\n",
       "      <td>0.404854</td>\n",
       "      <td>[lion, king, pretty, much, favourite, movie, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok, perhaps there is truth in the saying \"lost...</td>\n",
       "      <td>/title/tt0364569/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film begins in medias res, with the silhou...</td>\n",
       "      <td>0.115011</td>\n",
       "      <td>0.219557</td>\n",
       "      <td>[ok,, perhaps, truth, saying, lost, translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>top gun has been an 's staple since it first r...</td>\n",
       "      <td>/title/tt1745960/</td>\n",
       "      <td>1</td>\n",
       "      <td>Over three decades after his time at TOPGUN, C...</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>0.262304</td>\n",
       "      <td>[top, gun, s, staple, since, first, released, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will keep this brief: this is simply one of ...</td>\n",
       "      <td>/title/tt0060196/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film tells the story of three men who purs...</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>[keep, brief:, simply, one, entertaining, best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22874</th>\n",
       "      <td>let us be honest: is this film really worthy o...</td>\n",
       "      <td>/title/tt0211915/</td>\n",
       "      <td>0</td>\n",
       "      <td>Amelie Poulain (Audrey Tautou) is the only chi...</td>\n",
       "      <td>0.112264</td>\n",
       "      <td>0.048846</td>\n",
       "      <td>[let, us, honest:, film, really, worthy, top, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22875</th>\n",
       "      <td>paul edgecomb (tom hanks) is the lead guard on...</td>\n",
       "      <td>/title/tt0120689/</td>\n",
       "      <td>0</td>\n",
       "      <td>The movie opens with a group of people running...</td>\n",
       "      <td>0.270954</td>\n",
       "      <td>0.230906</td>\n",
       "      <td>[paul, edgecomb, (tom, hanks), lead, guard, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22876</th>\n",
       "      <td>this is undoubtedly the greatest film ever. th...</td>\n",
       "      <td>/title/tt0031381/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film opens in Tara, a cotton plantation ow...</td>\n",
       "      <td>0.106873</td>\n",
       "      <td>0.180562</td>\n",
       "      <td>[undoubtedly, greatest, film, ever., scale, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22877</th>\n",
       "      <td>no, this is not an horrible movie and i did no...</td>\n",
       "      <td>/title/tt0046268/</td>\n",
       "      <td>0</td>\n",
       "      <td>1950, in the very isolated town of Las Piedras...</td>\n",
       "      <td>0.174709</td>\n",
       "      <td>0.272613</td>\n",
       "      <td>[no,, horrible, movie, disliked, watching, abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22878</th>\n",
       "      <td>i had heard so much of this film, and how posi...</td>\n",
       "      <td>/title/tt0120382/</td>\n",
       "      <td>1</td>\n",
       "      <td>Truman Burbank ( Jim Carrey ) is an insurance ...</td>\n",
       "      <td>0.120066</td>\n",
       "      <td>0.351032</td>\n",
       "      <td>[heard, much, film,, positivly, rated, raved, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22879 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review              Title  \\\n",
       "0      the conclusion to the series hits some of the ...  /title/tt1201607/   \n",
       "1      the lion king was pretty much my favourite mov...  /title/tt0110357/   \n",
       "2      ok, perhaps there is truth in the saying \"lost...  /title/tt0364569/   \n",
       "3      top gun has been an 's staple since it first r...  /title/tt1745960/   \n",
       "4      i will keep this brief: this is simply one of ...  /title/tt0060196/   \n",
       "...                                                  ...                ...   \n",
       "22874  let us be honest: is this film really worthy o...  /title/tt0211915/   \n",
       "22875  paul edgecomb (tom hanks) is the lead guard on...  /title/tt0120689/   \n",
       "22876  this is undoubtedly the greatest film ever. th...  /title/tt0031381/   \n",
       "22877  no, this is not an horrible movie and i did no...  /title/tt0046268/   \n",
       "22878  i had heard so much of this film, and how posi...  /title/tt0120382/   \n",
       "\n",
       "       Spoiler_flag                                           Synopsis  \\\n",
       "0                 0  After burying Dobby at the garden of the Shell...   \n",
       "1                 1  The Lion King takes place in the Pride Lands o...   \n",
       "2                 0  The film begins in medias res, with the silhou...   \n",
       "3                 1  Over three decades after his time at TOPGUN, C...   \n",
       "4                 0  The film tells the story of three men who purs...   \n",
       "...             ...                                                ...   \n",
       "22874             0  Amelie Poulain (Audrey Tautou) is the only chi...   \n",
       "22875             0  The movie opens with a group of people running...   \n",
       "22876             0  The film opens in Tara, a cotton plantation ow...   \n",
       "22877             0  1950, in the very isolated town of Las Piedras...   \n",
       "22878             1  Truman Burbank ( Jim Carrey ) is an insurance ...   \n",
       "\n",
       "       Cosine_Similarity  doc_similarity  \\\n",
       "0               0.158318        0.106248   \n",
       "1               0.197134        0.404854   \n",
       "2               0.115011        0.219557   \n",
       "3               0.423765        0.262304   \n",
       "4               0.084172        0.156379   \n",
       "...                  ...             ...   \n",
       "22874           0.112264        0.048846   \n",
       "22875           0.270954        0.230906   \n",
       "22876           0.106873        0.180562   \n",
       "22877           0.174709        0.272613   \n",
       "22878           0.120066        0.351032   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [conclusion, series, hits, strongest, emotiona...  \n",
       "1      [lion, king, pretty, much, favourite, movie, g...  \n",
       "2      [ok,, perhaps, truth, saying, lost, translatio...  \n",
       "3      [top, gun, s, staple, since, first, released, ...  \n",
       "4      [keep, brief:, simply, one, entertaining, best...  \n",
       "...                                                  ...  \n",
       "22874  [let, us, honest:, film, really, worthy, top, ...  \n",
       "22875  [paul, edgecomb, (tom, hanks), lead, guard, de...  \n",
       "22876  [undoubtedly, greatest, film, ever., scale, fi...  \n",
       "22877  [no,, horrible, movie, disliked, watching, abs...  \n",
       "22878  [heard, much, film,, positivly, rated, raved, ...  \n",
       "\n",
       "[22879 rows x 7 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cab79e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "18e2c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.layers.normalization import batch_normalization\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bc8a1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"final_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0615f564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Review</th>\n",
       "      <th>Title</th>\n",
       "      <th>Spoiler_flag</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>doc_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21866</td>\n",
       "      <td>The conclusion to the series hits some of the ...</td>\n",
       "      <td>/title/tt1201607/</td>\n",
       "      <td>0</td>\n",
       "      <td>After burying Dobby at the garden of the Shell...</td>\n",
       "      <td>0.158318</td>\n",
       "      <td>0.106248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7319</td>\n",
       "      <td>The Lion King was pretty much my favourite mov...</td>\n",
       "      <td>/title/tt0110357/</td>\n",
       "      <td>1</td>\n",
       "      <td>The Lion King takes place in the Pride Lands o...</td>\n",
       "      <td>0.197134</td>\n",
       "      <td>0.404854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15075</td>\n",
       "      <td>OK, perhaps there is truth in the saying \"lost...</td>\n",
       "      <td>/title/tt0364569/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film begins in medias res, with the silhou...</td>\n",
       "      <td>0.115011</td>\n",
       "      <td>0.219557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3203</td>\n",
       "      <td>Top Gun has been an 80's staple since it first...</td>\n",
       "      <td>/title/tt1745960/</td>\n",
       "      <td>1</td>\n",
       "      <td>Over three decades after his time at TOPGUN, C...</td>\n",
       "      <td>0.423765</td>\n",
       "      <td>0.262304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10801</td>\n",
       "      <td>I'll keep this brief: This is simply one of th...</td>\n",
       "      <td>/title/tt0060196/</td>\n",
       "      <td>0</td>\n",
       "      <td>The film tells the story of three men who purs...</td>\n",
       "      <td>0.084172</td>\n",
       "      <td>0.156379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Review  \\\n",
       "0       21866  The conclusion to the series hits some of the ...   \n",
       "1        7319  The Lion King was pretty much my favourite mov...   \n",
       "2       15075  OK, perhaps there is truth in the saying \"lost...   \n",
       "3        3203  Top Gun has been an 80's staple since it first...   \n",
       "4       10801  I'll keep this brief: This is simply one of th...   \n",
       "\n",
       "               Title  Spoiler_flag  \\\n",
       "0  /title/tt1201607/             0   \n",
       "1  /title/tt0110357/             1   \n",
       "2  /title/tt0364569/             0   \n",
       "3  /title/tt1745960/             1   \n",
       "4  /title/tt0060196/             0   \n",
       "\n",
       "                                            Synopsis  Cosine_Similarity  \\\n",
       "0  After burying Dobby at the garden of the Shell...           0.158318   \n",
       "1  The Lion King takes place in the Pride Lands o...           0.197134   \n",
       "2  The film begins in medias res, with the silhou...           0.115011   \n",
       "3  Over three decades after his time at TOPGUN, C...           0.423765   \n",
       "4  The film tells the story of three men who purs...           0.084172   \n",
       "\n",
       "   doc_similarity  \n",
       "0        0.106248  \n",
       "1        0.404854  \n",
       "2        0.219557  \n",
       "3        0.262304  \n",
       "4        0.156379  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a907cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e1457109",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Review\"]\n",
    "y = df[\"Spoiler_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d96c1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9e2aec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_val, y_train_2, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c7f1438a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20591,)\n",
      "(2288,)\n",
      "(20591,)\n",
      "(2288,)\n",
      "(16472,)\n",
      "(4119,)\n",
      "(16472,)\n",
      "(4119,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train_2.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train_2.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f58e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        text = str(text)\n",
    "    return gensim.utils.simple_preprocess(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "db1a04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_train = X_train_2.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0e1ee196",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_test = X_val.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eb432389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285      [have, yet, to, watch, good, or, very, good, m...\n",
       "14544    [film, making, in, the, was, different, exerci...\n",
       "13400    [it, is, because, of, this, movie, that, began...\n",
       "6791     [marvel, seems, to, be, on, roll, towards, kil...\n",
       "1210     [ve, heard, good, things, around, this, movie,...\n",
       "                               ...                        \n",
       "11034    [what, can, be, said, about, this, film, that,...\n",
       "9141     [great, film, however, doesn, anyone, else, no...\n",
       "22403    [my, next, movie, review, is, going, to, be, o...\n",
       "17543    [guy, richie, follow, up, to, lock, stock, and...\n",
       "1296     [set, in, during, the, height, of, world, war,...\n",
       "Name: Review, Length: 16472, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7bfac5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Word2Vec Model\n",
    "w2v_num_features = 512\n",
    "w2v_model = gensim.models.Word2Vec(tokenize_train,size=w2v_num_features, window=150,min_count=2,sample=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2af079db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus,model,num_features):\n",
    "  vocabulary=set(model.wv.index2entity)\n",
    "\n",
    "  def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords=0\n",
    "    for word in words:\n",
    "      if word in vocabulary:\n",
    "        nwords=nwords+1\n",
    "        feature_vector=np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "      feature_vector = np.divide(feature_vector,nwords)\n",
    "\n",
    "    return feature_vector\n",
    "  features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n",
    "\n",
    "  return  np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a3234f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_train_features = document_vectorizer(corpus=tokenize_train,model=w2v_model, num_features=w2v_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9b8cd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_test_features = document_vectorizer(corpus=tokenize_test,model=w2v_model, num_features=w2v_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dfb692b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d15d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
    "from keras.engine import input_layer\n",
    "from keras.regularizers import l2\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "def construct_deepnn(num_input_features):\n",
    "  dnn_model = Sequential()\n",
    "  dnn_model.add(Dense(256, input_shape=(num_input_features,), kernel_initializer='glorot_uniform'))\n",
    "  dnn_model.add(BatchNormalization())\n",
    "  dnn_model.add(Activation('relu'))\n",
    "  dnn_model.add(Dropout(0.2))\n",
    "\n",
    "  dnn_model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01)))\n",
    "  dnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "  dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "02918f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9921638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000023121B90790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000023121B90790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "140/149 [===========================>..] - ETA: 0s - loss: 0.6580 - accuracy: 0.6538WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000023130C324C0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000023130C324C0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "149/149 [==============================] - 5s 34ms/step - loss: 0.6581 - accuracy: 0.6534 - val_loss: 0.6294 - val_accuracy: 0.6681\n",
      "Epoch 2/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.6041 - accuracy: 0.6846 - val_loss: 0.6122 - val_accuracy: 0.6675\n",
      "Epoch 3/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5896 - accuracy: 0.6957 - val_loss: 0.5994 - val_accuracy: 0.6808\n",
      "Epoch 4/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5816 - accuracy: 0.6969 - val_loss: 0.6011 - val_accuracy: 0.6784\n",
      "Epoch 5/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.6999 - val_loss: 0.6034 - val_accuracy: 0.6826\n",
      "Epoch 6/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.7054 - val_loss: 0.6005 - val_accuracy: 0.6857\n",
      "Epoch 7/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5669 - accuracy: 0.7080 - val_loss: 0.5994 - val_accuracy: 0.6839\n",
      "Epoch 8/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7162 - val_loss: 0.6079 - val_accuracy: 0.6802\n",
      "Epoch 9/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7122 - val_loss: 0.5997 - val_accuracy: 0.6857\n",
      "Epoch 10/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7167 - val_loss: 0.6091 - val_accuracy: 0.6808\n",
      "Epoch 11/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.7199 - val_loss: 0.6139 - val_accuracy: 0.6735\n",
      "Epoch 12/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7202 - val_loss: 0.6125 - val_accuracy: 0.6857\n",
      "Epoch 13/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7249 - val_loss: 0.6057 - val_accuracy: 0.6875\n",
      "Epoch 14/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5460 - accuracy: 0.7244 - val_loss: 0.6083 - val_accuracy: 0.6802\n",
      "Epoch 15/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7296 - val_loss: 0.6057 - val_accuracy: 0.6887\n",
      "Epoch 16/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7267 - val_loss: 0.6118 - val_accuracy: 0.6814\n",
      "Epoch 17/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5368 - accuracy: 0.7308 - val_loss: 0.6114 - val_accuracy: 0.6820\n",
      "Epoch 18/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7341 - val_loss: 0.6211 - val_accuracy: 0.6820\n",
      "Epoch 19/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7410 - val_loss: 0.6058 - val_accuracy: 0.6839\n",
      "Epoch 20/20\n",
      "149/149 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7410 - val_loss: 0.6197 - val_accuracy: 0.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23121baf040>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train_2, epochs=20, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "87a28302",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn.save(\"deep_learning1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6910b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dfa66c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counter = Counter([token for review in tokenize_train for token in review])\n",
    "\n",
    "vocab_map = {item[0]: index+1 for index,item in enumerate(dict(token_counter).items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eaaafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ea38b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 56221\n",
      "Sample Size of Vocabulary Size: {'every': 11, 'watched': 12, 'is': 13, 'also': 14, 'old': 15, 'jack': 16, 'nicholson': 17, 'as': 18, 'young': 19, 'film': 20}\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample Size of Vocabulary Size:', dict(list(vocab_map.items())[10:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "63bcad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max([len(review) for review in tokenize_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0ac23e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokeized train to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in line] for line in tokenize_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "65ff8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "93ccb3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = sequence.pad_sequences(train_X,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4014dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokeized test to numeric vectors\n",
    "\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX']for token in line] for line in tokenize_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d8b5ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = sequence.pad_sequences(test_X,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0da31a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop out 40%\n",
    "\n",
    "from keras.layers import Dropout, Embedding, Dense, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "\n",
    "EMBEDDING_DIMENSION = 128\n",
    "LSTM_DIM = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIMENSION, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.2,recurrent_dropout=0.4))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1a5a6e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002313147B0D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002313147B0D0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.5801WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000231316C6E50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000231316C6E50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "149/149 [==============================] - 899s 6s/step - loss: 0.6665 - accuracy: 0.5801 - val_loss: 0.6194 - val_accuracy: 0.6681\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 905s 6s/step - loss: 0.6069 - accuracy: 0.6813 - val_loss: 0.6301 - val_accuracy: 0.6511\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 915s 6s/step - loss: 0.4851 - accuracy: 0.7756 - val_loss: 0.6848 - val_accuracy: 0.6201\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 916s 6s/step - loss: 0.3517 - accuracy: 0.8529 - val_loss: 0.8304 - val_accuracy: 0.6420\n",
      "Epoch 5/10\n",
      "149/149 [==============================] - 914s 6s/step - loss: 0.2518 - accuracy: 0.9006 - val_loss: 0.9808 - val_accuracy: 0.6614\n",
      "Epoch 6/10\n",
      "149/149 [==============================] - 912s 6s/step - loss: 0.1786 - accuracy: 0.9352 - val_loss: 1.0441 - val_accuracy: 0.6256\n",
      "Epoch 7/10\n",
      "149/149 [==============================] - 923s 6s/step - loss: 0.1398 - accuracy: 0.9501 - val_loss: 1.1961 - val_accuracy: 0.6353\n",
      "Epoch 8/10\n",
      "149/149 [==============================] - 921s 6s/step - loss: 0.1007 - accuracy: 0.9664 - val_loss: 1.3092 - val_accuracy: 0.6062\n",
      "Epoch 9/10\n",
      "149/149 [==============================] - 917s 6s/step - loss: 0.0953 - accuracy: 0.9678 - val_loss: 1.4955 - val_accuracy: 0.6408\n",
      "Epoch 10/10\n",
      "149/149 [==============================] - 925s 6s/step - loss: 0.0757 - accuracy: 0.9731 - val_loss: 1.4461 - val_accuracy: 0.6050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231311cb880>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X,y_train_2,epochs=10, batch_size=100, shuffle=True, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9ec0e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11cd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
