{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b48b8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk as nt\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.layers.normalization import batch_normalization\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b2e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"final_df.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34bb619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\",inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ac1df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Review\"]\n",
    "y = df[\"Spoiler_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "deffb22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87b43faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_val, y_train_2, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b319c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        text = str(text)\n",
    "    return gensim.utils.simple_preprocess(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd741e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_train = X_train_2.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b53bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_test = X_val.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6adc9d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\noahr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nt.download('stopwords')\n",
    "sw = stopwords.words( 'english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b060e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    l = []\n",
    "    for i in text:\n",
    "        if i not in sw:\n",
    "            l.append(i.strip('\\'\\\"'))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31d3545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_train = tokenize_train.apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efa2a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_test = tokenize_test.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02c51a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Word2Vec Model\n",
    "w2v_num_features = 512\n",
    "w2v_model = gensim.models.Word2Vec(tokenize_train,size=w2v_num_features, window=150,min_count=2,sample=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6d3ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus,model,num_features):\n",
    "  vocabulary=set(model.wv.index2entity)\n",
    "\n",
    "  def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords=0\n",
    "    for word in words:\n",
    "      if word in vocabulary:\n",
    "        nwords=nwords+1\n",
    "        feature_vector=np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "      feature_vector = np.divide(feature_vector,nwords)\n",
    "\n",
    "    return feature_vector\n",
    "  features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features) for tokenized_sentence in corpus]\n",
    "\n",
    "  return  np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddf0429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_train_features = document_vectorizer(corpus=tokenize_train,model=w2v_model, num_features=w2v_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54454b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_wv_test_features = document_vectorizer(corpus=tokenize_test,model=w2v_model, num_features=w2v_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "798099d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5ada7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n",
    "from keras.engine import input_layer\n",
    "from keras.regularizers import l2\n",
    "from keras import regularizers\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "def construct_deepnn(num_input_features):\n",
    "  dnn_model = Sequential()\n",
    "  dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='glorot_uniform'))\n",
    "  dnn_model.add(BatchNormalization())\n",
    "  dnn_model.add(LeakyReLU(alpha=0.1))\n",
    "  dnn_model.add(Dropout(0.4))\n",
    "\n",
    "  dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "  dnn_model.add(BatchNormalization())\n",
    "  dnn_model.add(LeakyReLU(alpha=0.1))\n",
    "  dnn_model.add(Dropout(0.4))\n",
    "\n",
    "  dnn_model.add(Dense(512, kernel_initializer='glorot_uniform'))\n",
    "  dnn_model.add(BatchNormalization())\n",
    "  dnn_model.add(LeakyReLU(alpha=0.1))\n",
    "  dnn_model.add(Dropout(0.4))\n",
    "  \n",
    "\n",
    "  dnn_model.add(Dense(1))\n",
    "  dnn_model.add(Activation('sigmoid'))\n",
    "\n",
    "  dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8b7ef0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e62da5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "149/149 [==============================] - 2s 11ms/step - loss: 0.7040 - accuracy: 0.6239 - val_loss: 0.6127 - val_accuracy: 0.6687\n",
      "Epoch 2/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.6388 - accuracy: 0.6525 - val_loss: 0.6006 - val_accuracy: 0.6917\n",
      "Epoch 3/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.6168 - accuracy: 0.6618 - val_loss: 0.6198 - val_accuracy: 0.6772\n",
      "Epoch 4/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.6075 - accuracy: 0.6720 - val_loss: 0.6035 - val_accuracy: 0.6911\n",
      "Epoch 5/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.6009 - accuracy: 0.6777 - val_loss: 0.6028 - val_accuracy: 0.6760\n",
      "Epoch 6/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5941 - accuracy: 0.6855 - val_loss: 0.5959 - val_accuracy: 0.6875\n",
      "Epoch 7/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5901 - accuracy: 0.6892 - val_loss: 0.6100 - val_accuracy: 0.6784\n",
      "Epoch 8/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.5908 - accuracy: 0.6889 - val_loss: 0.5930 - val_accuracy: 0.6851\n",
      "Epoch 9/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.5877 - accuracy: 0.6913 - val_loss: 0.5925 - val_accuracy: 0.6857\n",
      "Epoch 10/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.5845 - accuracy: 0.6977 - val_loss: 0.5965 - val_accuracy: 0.6978\n",
      "Epoch 11/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5841 - accuracy: 0.6939 - val_loss: 0.5955 - val_accuracy: 0.6984\n",
      "Epoch 12/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.5804 - accuracy: 0.6924 - val_loss: 0.5879 - val_accuracy: 0.6917\n",
      "Epoch 13/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5811 - accuracy: 0.6976 - val_loss: 0.5985 - val_accuracy: 0.6857\n",
      "Epoch 14/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5822 - accuracy: 0.6981 - val_loss: 0.6017 - val_accuracy: 0.6742\n",
      "Epoch 15/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5789 - accuracy: 0.6978 - val_loss: 0.5991 - val_accuracy: 0.6863\n",
      "Epoch 16/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5765 - accuracy: 0.7020 - val_loss: 0.5944 - val_accuracy: 0.6845\n",
      "Epoch 17/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5736 - accuracy: 0.7053 - val_loss: 0.5893 - val_accuracy: 0.6887\n",
      "Epoch 18/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5740 - accuracy: 0.7040 - val_loss: 0.5917 - val_accuracy: 0.6984\n",
      "Epoch 19/20\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 0.5699 - accuracy: 0.7042 - val_loss: 0.5984 - val_accuracy: 0.6857\n",
      "Epoch 20/20\n",
      "149/149 [==============================] - 1s 10ms/step - loss: 0.5709 - accuracy: 0.7034 - val_loss: 0.5854 - val_accuracy: 0.7063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f2b9a4850>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "w2v_dnn.fit(avg_wv_train_features, y_train_2, epochs=20, batch_size=batch_size, shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3a3dbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn.save(\"deep_learning2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "826ee6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "227106d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counter = Counter([token for review in tokenize_train for token in review])\n",
    "\n",
    "vocab_map = {item[0]: index+1 for index,item in enumerate(dict(token_counter).items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "db25ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.max(list(vocab_map.values()))\n",
    "vocab_map['PAD_INDEX'] = 0\n",
    "vocab_map['NOT_FOUND_INDEX'] = max_index+1\n",
    "vocab_size = len(vocab_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c76589c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 56076\n",
      "Sample Size of Vocabulary Size: {'jack': 11, 'nicholson': 12, 'young': 13, 'film': 14, 'making': 15, 'different': 16, 'exercise': 17, 'today': 18, 'putting': 19, 'show': 20}\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Sample Size of Vocabulary Size:', dict(list(vocab_map.items())[10:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1e8888ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max([len(review) for review in tokenize_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "add9e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokeized train to numeric vectors\n",
    "train_X = [[vocab_map[token] for token in line] for line in tokenize_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "053be137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "07fc0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = sequence.pad_sequences(train_X,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc37f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokeized test to numeric vectors\n",
    "\n",
    "test_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX']for token in line] for line in tokenize_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3fefad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = sequence.pad_sequences(test_X,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5bbb7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "EMBEDDING_DIMENSION = 256\n",
    "LSTM_DIM = 64\n",
    "REGULARIZATION_RATE = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIMENSION, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(LSTM_DIM, dropout=0.4, recurrent_dropout=0.4, kernel_regularizer=l2(REGULARIZATION_RATE)))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(REGULARIZATION_RATE)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ffba2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "149/149 [==============================] - 491s 3s/step - loss: 1.2008 - accuracy: 0.6073 - val_loss: 0.6178 - val_accuracy: 0.6966\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 487s 3s/step - loss: 0.5949 - accuracy: 0.7190 - val_loss: 0.6101 - val_accuracy: 0.6899\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 488s 3s/step - loss: 0.5005 - accuracy: 0.8003 - val_loss: 0.6627 - val_accuracy: 0.6426\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 487s 3s/step - loss: 0.4501 - accuracy: 0.8373 - val_loss: 0.6962 - val_accuracy: 0.6887\n",
      "Epoch 5/10\n",
      " 79/149 [==============>...............] - ETA: 3:43 - loss: 0.3742 - accuracy: 0.8796"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14496/2357168777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_X,y_train_2,epochs=10, batch_size=100, shuffle=True, validation_split=0.1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e7bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
